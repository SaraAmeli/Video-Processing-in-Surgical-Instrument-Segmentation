{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23195,"status":"ok","timestamp":1744987606949,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"},"user_tz":-120},"id":"xqsN__gUhrhF","outputId":"174902de-43ff-474b-e473-189f872317e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!ls /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvWm4aVoXmRM","executionInfo":{"status":"ok","timestamp":1744987648039,"user_tz":-120,"elapsed":792,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"}},"outputId":"16a49111-aa98-4e3f-9d36-cf36511affb9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[" 0_0_addtochaos_0-20250416T234111Z-001.zip  'Colab Notebooks'   Image_Vision\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136145,"status":"ok","timestamp":1744987122743,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"},"user_tz":-120},"id":"qllU84e0QNNB","outputId":"a083f834-db7f-4b20-8b37-8e84c37ed4eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully extracted test set!\n"]}],"source":["import zipfile\n","import os\n","\n","zip_path = \"/mnt/gdrive/MyDrive/Image_Vision/24932499.zip\"\n","extract_to = \"/mnt/gdrive/MyDrive/Image_Vision/test_set\"\n","\n","os.makedirs(extract_to, exist_ok=True)\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","\n","print(\"✅ Successfully extracted test set!\")"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":171461,"status":"ok","timestamp":1744910489903,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"},"user_tz":-120},"id":"2UiaALL_hk7S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc44173f-6409-42cc-d526-feefaaa936c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔓 Unzipping video_41.zip...\n","✅ Done: video_41\n","🔓 Unzipping video_42.zip...\n","✅ Done: video_42\n","🔓 Unzipping video_43.zip...\n","✅ Done: video_43\n","🔓 Unzipping video_44.zip...\n","✅ Done: video_44\n","🔓 Unzipping video_45.zip...\n","✅ Done: video_45\n","🔓 Unzipping video_46.zip...\n","✅ Done: video_46\n","🔓 Unzipping video_47.zip...\n","✅ Done: video_47\n","🔓 Unzipping video_48.zip...\n","✅ Done: video_48\n","🔓 Unzipping video_49.zip...\n","✅ Done: video_49\n","🔓 Unzipping video_50.zip...\n","✅ Done: video_50\n"]}],"source":["# Unzip test videos\n","import os\n","import zipfile\n","\n","zip_dir = \"/mnt/gdrive/MyDrive/Image_Vision/test_set\"\n","output_dir = \"/mnt/gdrive/MyDrive/Image_Vision/test_videos\"\n","\n","os.makedirs(output_dir, exist_ok=True)\n","\n","for file in sorted(os.listdir(zip_dir)):\n","    if file.endswith(\".zip\") and file.startswith(\"video_\"):\n","        zip_path = os.path.join(zip_dir, file)\n","        folder_name = file.replace(\".zip\", \"\")\n","        extract_path = os.path.join(output_dir, folder_name)\n","\n","        if not os.path.exists(extract_path):\n","            print(f\"🔓 Unzipping {file}...\")\n","            try:\n","                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","                    zip_ref.extractall(extract_path)\n","                print(f\"✅ Done: {folder_name}\")\n","            except zipfile.BadZipFile:\n","                print(f\"❌ Skipping {file} — not a valid zip.\")\n","        else:\n","            print(f\"🟡 Already unzipped: {folder_name}\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-myIfa8Ph7U-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744910591258,"user_tz":-120,"elapsed":105,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"}},"outputId":"d72532f2-1546-43ad-f09f-a7bb6b95e214"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 473M\n","-rw------- 1 root root  406 Apr 17 17:18 action_continuous.txt\n","-rw------- 1 root root  28K Apr 17 17:18 action_discrete.txt\n","drwx------ 2 root root 4.0K Apr 17 17:18 segmentation\n","-rw------- 1 root root 473M Apr 17 17:18 video_left.avi\n"]}],"source":["# Check whats inside\n","!ls -lh /mnt/gdrive/MyDrive/Image_Vision/test_videos/video_41"]},{"cell_type":"code","source":["#Extract Frames from All Test Videos\n","import cv2\n","import os\n","\n","def extract_frames(video_path, output_dir, every_n=5):\n","    os.makedirs(output_dir, exist_ok=True)\n","    cap = cv2.VideoCapture(video_path)\n","    frame_idx = 0\n","    saved_idx = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if frame_idx % every_n == 0:\n","            out_path = os.path.join(output_dir, f\"{frame_idx:09d}.jpg\")\n","            cv2.imwrite(out_path, frame)\n","            saved_idx += 1\n","        frame_idx += 1\n","\n","    cap.release()\n","    print(f\"✅ Extracted {saved_idx} frames from {os.path.basename(video_path)}\")\n","\n","# 🔁 Batch extract frames from all test videos\n","video_base = \"/mnt/gdrive/MyDrive/Image_Vision/test_videos\"\n","output_base = \"/mnt/gdrive/MyDrive/Image_Vision/test_frames\"\n","\n","for folder in sorted(os.listdir(video_base)):\n","    video_path = os.path.join(video_base, folder, \"video_left.avi\")\n","    output_dir = os.path.join(output_base, folder)\n","\n","    if os.path.exists(video_path):\n","        print(f\"🎞 Extracting frames from: {folder}\")\n","        extract_frames(video_path, output_dir, every_n=5)\n","    else:\n","        print(f\"⚠️ Missing video in {folder}, skipping...\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyzxvfQZxqPe","executionInfo":{"status":"ok","timestamp":1744913211522,"user_tz":-120,"elapsed":2515002,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"}},"outputId":"fae94402-cc70-4675-a859-75f14059ee20"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["🎞 Extracting frames from: video_41\n","✅ Extracted 2856 frames from video_left.avi\n","🎞 Extracting frames from: video_42\n","✅ Extracted 4289 frames from video_left.avi\n","🎞 Extracting frames from: video_43\n","✅ Extracted 2371 frames from video_left.avi\n","🎞 Extracting frames from: video_44\n","✅ Extracted 1715 frames from video_left.avi\n","🎞 Extracting frames from: video_45\n","✅ Extracted 1395 frames from video_left.avi\n","🎞 Extracting frames from: video_46\n","✅ Extracted 7586 frames from video_left.avi\n","🎞 Extracting frames from: video_47\n","✅ Extracted 3999 frames from video_left.avi\n","🎞 Extracting frames from: video_48\n","✅ Extracted 3602 frames from video_left.avi\n","🎞 Extracting frames from: video_49\n","✅ Extracted 8237 frames from video_left.avi\n","🎞 Extracting frames from: video_50\n","✅ Extracted 2916 frames from video_left.avi\n"]}]},{"cell_type":"code","source":["#Load Trained Model\n","\n","import torch\n","\n","import torch.nn as nn\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_classes):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","                nn.ReLU(inplace=True),\n","            )\n","\n","        self.down1 = conv_block(3, 64)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.down2 = conv_block(64, 128)\n","        self.pool2 = nn.MaxPool2d(2)\n","        self.down3 = conv_block(128, 256)\n","        self.pool3 = nn.MaxPool2d(2)\n","\n","        self.middle = conv_block(256, 512)\n","\n","        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n","        self.upconv3 = conv_block(512, 256)\n","        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n","        self.upconv2 = conv_block(256, 128)\n","        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n","        self.upconv1 = conv_block(128, 64)\n","\n","        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        d1 = self.down1(x)\n","        p1 = self.pool1(d1)\n","        d2 = self.down2(p1)\n","        p2 = self.pool2(d2)\n","        d3 = self.down3(p2)\n","        p3 = self.pool3(d3)\n","\n","        m = self.middle(p3)\n","\n","        u3 = self.up3(m)\n","        u3 = self.upconv3(torch.cat([u3, d3], dim=1))\n","        u2 = self.up2(u3)\n","        u2 = self.upconv2(torch.cat([u2, d2], dim=1))\n","        u1 = self.up1(u2)\n","        u1 = self.upconv1(torch.cat([u1, d1], dim=1))\n","\n","        return self.final(u1)\n","\n","\n","\n","# Load model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"⚡ Using device:\", device)\n","\n","\n","model_path = \"/mnt/gdrive/MyDrive/Image_Vision/models/best_unet.pth\"\n","\n","model = UNet(n_classes=10).to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Image_Vision/models/best_unet.pth\"))\n","model.eval()\n","print(\"✅ Model loaded!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOp19yV-yD-D","executionInfo":{"status":"ok","timestamp":1744987859147,"user_tz":-120,"elapsed":124,"user":{"displayName":"Sara Ameli","userId":"03676648058668107608"}},"outputId":"2a5c7c72-130d-4f68-e102-1bf8f77c49dc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["⚡ Using device: cuda\n","✅ Model loaded!\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision import transforms\n","\n","from PIL import Image\n","\n","# --- Setup ---\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = UNet(n_classes=10).to(device)  # 🔁 make sure UNet is already defined\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Image_Vision/models/best_unet.pth\"))\n","model.eval()\n","print(\"✅ Model loaded!\")\n","\n","# --- Paths ---\n","test_videos = [f\"video_{i}\" for i in range(41, 51)]\n","test_frames_root = \"/content/drive/MyDrive/Image_Vision/test_frames\"\n","output_root = \"/content/drive/MyDrive/Image_Vision/test_predictions\"\n","rgb_preview_root = \"/content/drive/MyDrive/Image_Vision/test_predictions_rgb\"  # optional\n","\n","# --- Label to color map for preview visualization ---\n","label_to_color = {\n","    0: (0, 0, 0),         # background\n","    1: (255, 0, 0),\n","    2: (0, 255, 0),\n","    3: (0, 0, 255),\n","    4: (255, 255, 0),\n","    5: (0, 255, 255),\n","    6: (128, 0, 255),\n","    7: (255, 0, 255),\n","    8: (0, 128, 255),\n","    9: (255, 255, 255)\n","}\n","\n","# --- Frame prediction ---\n","def predict_frame(img_path):\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img, (384, 384))\n","    tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(tensor)\n","        pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)\n","\n","    return pred  # raw class mask (0-9)\n","\n","# --- Optional: Convert class mask to RGB for preview ---\n","def mask_to_rgb(mask):\n","    rgb = np.zeros((384, 384, 3), dtype=np.uint8)\n","    for label, color in label_to_color.items():\n","        rgb[mask == label] = color\n","    return rgb\n","\n","# --- Run predictions ---\n","for video in test_videos:\n","    print(f\"🎯 Predicting for {video}\")\n","    input_dir = os.path.join(test_frames_root, video)\n","    output_dir = os.path.join(output_root, video)\n","    preview_dir = os.path.join(rgb_preview_root, video)\n","    os.makedirs(output_dir, exist_ok=True)\n","    os.makedirs(preview_dir, exist_ok=True)\n","\n","    frame_list = sorted([f for f in os.listdir(input_dir) if f.endswith(\".jpg\")])\n","\n","    for frame_name in tqdm(frame_list):\n","        img_path = os.path.join(input_dir, frame_name)\n","        pred_mask = predict_frame(img_path)\n","\n","        # Save raw class mask (for evaluation)\n","        out_path = os.path.join(output_dir, frame_name.replace(\".jpg\", \".png\"))\n","        cv2.imwrite(out_path, pred_mask)\n","\n","        # Save RGB mask (for preview)\n","        rgb_mask = mask_to_rgb(pred_mask)\n","        preview_path = os.path.join(preview_dir, frame_name.replace(\".jpg\", \".png\"))\n","        Image.fromarray(rgb_mask).save(preview_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRY08QYzW7nX","outputId":"f085a6d0-17eb-492b-d635-91e75013d15a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model loaded!\n","🎯 Predicting for video_41\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2856/2856 [02:10<00:00, 21.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["🎯 Predicting for video_42\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 4261/4289 [18:17<00:16,  1.68it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"E_7U_Sxro56j"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOtswu3WMLX6Q/3lD5RV4DL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}